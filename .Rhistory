DocumentTermMatrix(control = list(tokenize = bigram.tokenizer))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 10, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "books", "chapter", "newbery",
"newberry", "award", "john", "medal",
"winner", "writer", "winners", )
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "books", "chapter", "newbery",
"newberry", "award", "john", "medal",
"winner", "writer", "winners", )
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "books", "chapter", "newbery",
"newberry", "award", "john", "medal",
"winner", "writer", "winners")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
?DocumentTermMatrix
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
colnames(as.matrix(dtm_reduced))
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
max_doc_topics <- apply(lda_model@gamma, 1, which.max)
reviews$Book[max_doc_topics == 1]
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
colnames(as.matrix(dtm_reduced))
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award",
"literature")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award",
"literature", "medal")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
max_doc_topics <- apply(lda_model@gamma, 1, which.max)
reviews$Book[max_doc_topics == 1]
max_doc_topics <- apply(lda_model@gamma, 1, which.max)
reviews$Book[max_doc_topics == 1]
reviews$Book[max_doc_topics == 2]
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award",
"literature", "medal", "recommend")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
max_doc_topics <- apply(lda_model@gamma, 1, which.max)
reviews$Book[max_doc_topics == 1]
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award",
"literature", "medal", "recommend", "recommended")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
max_doc_topics <- apply(lda_model@gamma, 1, which.max)
reviews$Book[max_doc_topics == 1]
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award",
"literature", "medal", "recommend", "recommended",
"recommends")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
reviews$Book[max_doc_topics == 2]
reviews$Book[max_doc_topics == 3]
reviews$Book[max_doc_topics == 3]
reviews$Book[max_doc_topics == 4]
reviews$Book[max_doc_topics == 5]
reviews$Book[max_doc_topics == 6]
reviews$Book[max_doc_topics == 2]
reviews$Book[max_doc_topics == 6]
reviews$Book[max_doc_topics == 2]
reviews$Book[max_doc_topics == 6]
reviews$Book[max_doc_topics == 6]
# Chunk 1: "setup"
knitr::opts_knit$set(base.dir = "C:/Users/david/Documents/Website/darthur2.github.io", base.url = "/")
knitr::opts_knit$set(root.dir = "/Users/david/Documents/Website/darthur2.github.io", base.url = "/")
knitr::opts_chunk$set(fig.path = "images/lda1-")
# Chunk 2
suppressMessages({
suppressWarnings({
library(openxlsx)
library(topicmodels)
library(tokenizers)
library(textmineR)
library(tm)
library(tidyverse)
library(tidytext)
library(xtable)
library(Matrix)
})
})
# Chunk 3
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
substr(reviews$Reviews[1:3], 1, 500)
# Chunk 4
avenger_words <- cbind(c("america", "justice", "freedom"),
c("punch", "kick", "justice"),
c("tech", "gadgets", "justice"),
c("thanos", "government", "justice"))
colnames(avenger_words) <- c("Topic 1", "Topic 2",
"Topic 3", "Topic 4")
rownames(avenger_words) <- c("Word 1", "Word 2", "Word 3")
avenger_xtable <- xtable(avenger_words)
# Chunk 5
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1", "ASCII", sub = " ")
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% Corpus()
newbery_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(removeNumbers = TRUE,
stopwords = stop_words$word,
stemming = FALSE,
removePunctuation = TRUE,
tolower = TRUE))
dtm_matrix <- as.matrix(newbery_dtm)
termdoc_freqs <- TermDocFreq(dtm_matrix)
remove_words <- termdoc_freqs %>%
filter(term_freq < 20 | doc_freq < 15) %>%
select(term)
my_stopwords <- c(stop_words$word, remove_words$term,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award",
"literature", "medal", "recommend", "recommended",
"recommends")
review_words <- tokenize_words(reviews$Reviews,
stopwords = my_stopwords,
strip_numeric = TRUE) %>%
sapply(., paste, collapse = " ")
substr(review_words[1:3], 1, 500)
# Chunk 6
newbery_dtm2 <- newbery_corpus %>%
DocumentTermMatrix(control = list(removeNumbers = TRUE,
stopwords = my_stopwords,
stemming = TRUE,
removePunctuation = TRUE,
tolower = TRUE))
lda_model <- LDA(newbery_dtm2, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
# Chunk 7
max_doc_topics <- apply(lda_model@gamma, 1, which.max)
reviews$Book[max_doc_topics == 1]
# Chunk 8
reviews$Book[max_doc_topics == 2]
# Chunk 9
reviews$Book[max_doc_topics == 3]
# Chunk 10
reviews$Book[max_doc_topics == 4]
# Chunk 11
reviews$Book[max_doc_topics == 5]
# Chunk 12
reviews$Book[max_doc_topics == 6]
reviews$Book[max_doc_topics == 1]
# Chunk 1: "setup"
knitr::opts_knit$set(base.dir = "C:/Users/david/Documents/Website/darthur2.github.io", base.url = "/")
knitr::opts_knit$set(root.dir = "/Users/david/Documents/Website/darthur2.github.io", base.url = "/")
knitr::opts_chunk$set(fig.path = "images/lda2-")
# Chunk 2
suppressMessages({
suppressWarnings({
library(openxlsx)
library(topicmodels)
library(tokenizers)
library(textmineR)
library(tm)
library(tidyverse)
library(tidytext)
library(xtable)
library(Matrix)
})
})
# Chunk 3
bigram.tokenizer <- function(x){
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "),
use.names = FALSE)
}
# Chunk 4
reviews_file <- "files/book_reviews.xlsx"
reviews <- read.xlsx(reviews_file)
author_names <- tokenize_words(reviews$Author) %>% unlist()
book_titles <- tokenize_words(reviews$Book) %>% unlist()
my_stopwords <- c(stop_words$word, author_names, book_titles,
"book", "chapter", "newbery", "story", "stories",
"newberry", "winners", "writer", "reads", "winner",
"books", "read", "reader", "read", "reading",
"character", "characters", "john", "award",
"literature", "medal", "recommend", "recommended",
"recommends")
latin1_apstrphe <- iconv("\u0092", "UTF-8", to = "latin1")
reviews$Reviews <- gsub(latin1_apstrphe, "'", reviews$Reviews)
reviews$Reviews <- iconv(reviews$Reviews, "latin1",
"ASCII", sub = " ") %>%
gsub("\n", " ", .) %>% tolower() %>%
removeWords(my_stopwords) %>% removeNumbers() %>%
removePunctuation() %>% stripWhitespace()
newbery_corpus <- reviews$Reviews %>% VectorSource() %>% VCorpus()
reviews_dtm <- newbery_corpus %>%
DocumentTermMatrix(control = list(tokenize = bigram.tokenizer,
stemming = TRUE))
dtm_reduced <- removeSparseTerms(reviews_dtm, 0.96)
lda_model <- LDA(dtm_reduced, k = 6, control = list(seed = 100))
reviews_topics <- tidy(lda_model, matrix = "beta")
top_terms <- reviews_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
# Chunk 5
max_doc_topics <- apply(lda_model@gamma, 1, which.max)
reviews$Book[max_doc_topics == 1]
# Chunk 6
reviews$Book[max_doc_topics == 2]
# Chunk 7
reviews$Book[max_doc_topics == 3]
# Chunk 8
reviews$Book[max_doc_topics == 4]
# Chunk 9
reviews$Book[max_doc_topics == 5]
# Chunk 10
reviews$Book[max_doc_topics == 6]
reviews$Book
lda_model@gamma[64,]
